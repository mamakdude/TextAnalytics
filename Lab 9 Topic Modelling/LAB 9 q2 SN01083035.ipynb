{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fab459e-e052-4485-bd91-8b2e5f2fae10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0950ce30-b0ce-42e4-9f6c-0376f6201f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv('npr.csv')  \n",
    "documents = df['Article'].tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3082335-6b5d-4885-9dab-ff331d73781f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [t for t in tokens if t.isalnum()]\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    return tokens\n",
    "\n",
    "pre_docs = [preprocess(doc) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b973f446-dcd0-4317-b7fd-76e590ac9da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Document-Term Matrix\n",
    "dictionary = corpora.Dictionary(pre_docs)\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in pre_docs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d077c925-685d-4d44-9d2d-65d51f1222b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run LDA Model\n",
    "lda_model = LdaModel(corpus, num_topics=5, id2word=dictionary, passes=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8420b5ab-44b9-46b6-8893-f4ddc0f922fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign Topics to Each Document\n",
    "article_labels = []\n",
    "for doc in pre_docs:\n",
    "    bow = dictionary.doc2bow(doc)\n",
    "    topics = lda_model.get_document_topics(bow)\n",
    "    dominant_topic = max(topics, key=lambda x: x[1])[0]\n",
    "    article_labels.append(dominant_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68be8ff8-df64-4a86-9fd5-ff18805a2aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create Result DataFrame\n",
    "df_result = pd.DataFrame({\"Article\": documents, \"Topic\": article_labels})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38d69f8-6680-4c63-bc3f-8633e253c5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Articles with Assigned Topics\n",
    "print(\"Table with Articles and Assigned Topics:\")\n",
    "print(df_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68746c19-99d9-4b9e-9a0a-75016c9f18f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Top Terms for Each Topic\n",
    "print(\"\\nTop Terms for Each Topic:\")\n",
    "for topic_id in range(lda_model.num_topics):\n",
    "    print(f\"Topic #{topic_id}:\")\n",
    "    top_terms = lda_model.show_topic(topic_id, topn=10)\n",
    "    print([term[0] for term in top_terms])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f2589d71-2bfe-4da4-8eab-958d78db0a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Terms with Weights:\n",
      "Topic 0:\n",
      "- \"police\" (weight: 0.007)\n",
      "- \"report\" (weight: 0.006)\n",
      "- \"state\" (weight: 0.005)\n",
      "- \"government\" (weight: 0.005)\n",
      "- \"country\" (weight: 0.005)\n",
      "- \"court\" (weight: 0.004)\n",
      "- \"law\" (weight: 0.004)\n",
      "- \"told\" (weight: 0.004)\n",
      "- \"attack\" (weight: 0.004)\n",
      "- \"official\" (weight: 0.004)\n",
      "\n",
      "Topic 1:\n",
      "- \"health\" (weight: 0.010)\n",
      "- \"school\" (weight: 0.008)\n",
      "- \"child\" (weight: 0.006)\n",
      "- \"student\" (weight: 0.006)\n",
      "- \"study\" (weight: 0.006)\n",
      "- \"care\" (weight: 0.006)\n",
      "- \"percent\" (weight: 0.005)\n",
      "- \"woman\" (weight: 0.004)\n",
      "- \"state\" (weight: 0.004)\n",
      "- \"program\" (weight: 0.004)\n",
      "\n",
      "Topic 2:\n",
      "- \"know\" (weight: 0.005)\n",
      "- \"think\" (weight: 0.005)\n",
      "- \"thing\" (weight: 0.005)\n",
      "- \"life\" (weight: 0.005)\n",
      "- \"really\" (weight: 0.004)\n",
      "- \"woman\" (weight: 0.004)\n",
      "- \"story\" (weight: 0.004)\n",
      "- \"show\" (weight: 0.003)\n",
      "- \"world\" (weight: 0.003)\n",
      "- \"book\" (weight: 0.003)\n",
      "\n",
      "Topic 3:\n",
      "- \"food\" (weight: 0.007)\n",
      "- \"water\" (weight: 0.005)\n",
      "- \"company\" (weight: 0.005)\n",
      "- \"world\" (weight: 0.004)\n",
      "- \"country\" (weight: 0.003)\n",
      "- \"million\" (weight: 0.003)\n",
      "- \"city\" (weight: 0.003)\n",
      "- \"000\" (weight: 0.003)\n",
      "- \"area\" (weight: 0.003)\n",
      "- \"percent\" (weight: 0.003)\n",
      "\n",
      "Topic 4:\n",
      "- \"trump\" (weight: 0.030)\n",
      "- \"clinton\" (weight: 0.012)\n",
      "- \"president\" (weight: 0.010)\n",
      "- \"state\" (weight: 0.009)\n",
      "- \"republican\" (weight: 0.008)\n",
      "- \"campaign\" (weight: 0.008)\n",
      "- \"election\" (weight: 0.006)\n",
      "- \"obama\" (weight: 0.006)\n",
      "- \"vote\" (weight: 0.006)\n",
      "- \"house\" (weight: 0.005)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Top Terms with Weights:\")\n",
    "for idx, topic in lda_model.print_topics():\n",
    "    print(f\"Topic {idx}:\")\n",
    "    terms = [term.strip() for term in topic.split(\"+\")]\n",
    "    for term in terms:\n",
    "        weight, word = term.split(\"*\")\n",
    "        print(f\"- {word.strip()} (weight: {weight.strip()})\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
