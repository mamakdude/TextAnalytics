{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67803656-7945-4ada-93de-c3c74f680270",
   "metadata": {},
   "source": [
    "### TEXT CLUSTERING USING TF-IDF VECTORIZER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8b3ba629-55d1-45ab-93d4-cfe148dac624",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tabulate import tabulate\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "52f8de2e-8283-494a-be56-6342916d2014",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\"I love playing football on the weekends\",\n",
    " \"I enjoy hiking and camping in the mountains\",\n",
    " \"I like to read books and watch movies\",\n",
    " \"I prefer playing video games over sports\",\n",
    " \"I love listening to music and going to concerts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "fdd40a43-1a80-4f23-9e58-8591b6168668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love playing football weekend', 'enjoy hiking camping mountain', 'like read book watch movie', 'prefer playing video game sport', 'love listening music going concert']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Init tools\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Lowercase and remove punctuation/URLs\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(r\"[^a-z\\s]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    # Tokenize, remove stopwords, lemmatize\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Preprocess\n",
    "preprocessed_dataset = [preprocess_text(text) for text in dataset]\n",
    "\n",
    "print(preprocessed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "752a95b3-bd00-4685-888d-a2916af14b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(preprocessed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "be585d85-0d51-4a7f-922d-3d832c1bd845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document                              Predicted Cluster\n",
      "----------------------------------  -------------------\n",
      "love playing football weekend                         1\n",
      "enjoy hiking camping mountain                         0\n",
      "like read book watch movie                            1\n",
      "prefer playing video game sport                       1\n",
      "love listening music going concert                    1\n",
      "\n",
      "Top terms per cluster:\n",
      "Cluster 0:\n",
      " camping\n",
      "\n",
      " enjoy\n",
      "\n",
      " hiking\n",
      "\n",
      " mountain\n",
      "\n",
      " weekend\n",
      "\n",
      " listening\n",
      "\n",
      " concert\n",
      "\n",
      " football\n",
      "\n",
      " game\n",
      "\n",
      " going\n",
      "\n",
      "Cluster 1:\n",
      " love\n",
      "\n",
      " playing\n",
      "\n",
      " football\n",
      "\n",
      " weekend\n",
      "\n",
      " going\n",
      "\n",
      " sport\n",
      "\n",
      " music\n",
      "\n",
      " concert\n",
      "\n",
      " video\n",
      "\n",
      " game\n",
      "\n"
     ]
    }
   ],
   "source": [
    "k = 2 # Define the number of clusters\n",
    "km = KMeans(n_clusters=k)\n",
    "km.fit(X)\n",
    "\n",
    "# Predict the clusters for each document\n",
    "y_pred = km.predict(X)\n",
    "\n",
    "# Display the document and its predicted cluster in a table\n",
    "table_data = [[\"Document\", \"Predicted Cluster\"]]\n",
    "table_data.extend([[doc, cluster] for doc, cluster in zip(preprocessed_dataset, y_pred)])\n",
    "print(tabulate(table_data, headers=\"firstrow\"))\n",
    "\n",
    "# Print top terms per cluster\n",
    "print(\"\\nTop terms per cluster:\")\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "for i in range(k):\n",
    " print(\"Cluster %d:\" % i)\n",
    " for ind in order_centroids[i, :10]:\n",
    "  print(' %s' % terms[ind])\n",
    "  print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a051e3ff-7de0-407c-a66e-d968b387d629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purity: 0.8\n"
     ]
    }
   ],
   "source": [
    "# Calculate purity\n",
    "total_samples = len(y_pred)\n",
    "cluster_label_counts = [Counter(y_pred)]\n",
    "purity = sum(max(cluster.values()) for cluster in cluster_label_counts) / total_samples\n",
    "print(\"Purity:\", purity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee47416-366e-481b-8bfe-777800b35785",
   "metadata": {},
   "source": [
    "### TEXT CLUSTERING USING WORD2VEC VECTORIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f1777e06-77ca-432e-8b29-38c400e2b570",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from gensim.models import Word2Vec\n",
    "from tabulate import tabulate\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8be6ab6d-8515-4bef-a356-2e36a0d2fafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = [doc.split() for doc in preprocessed_dataset]\n",
    "word2vec_model = Word2Vec(sentences=tokenized_dataset, vector_size=100,\n",
    "window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d20e6897-1e18-46e7-9fc0-96fa97634a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([np.mean([word2vec_model.wv[word] for word in doc.split() if word in\n",
    "word2vec_model.wv], axis=0) for doc in dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "766e5a88-490c-4fd6-9f59-184ac8883af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document                              Predicted Cluster\n",
      "----------------------------------  -------------------\n",
      "love playing football weekend                         0\n",
      "enjoy hiking camping mountain                         1\n",
      "like read book watch movie                            1\n",
      "prefer playing video game sport                       0\n",
      "love listening music going concert                    1\n"
     ]
    }
   ],
   "source": [
    "k = 2 # Define the number of clusters\n",
    "km = KMeans(n_clusters=k)\n",
    "km.fit(X)\n",
    "\n",
    "# Predict the clusters for each document\n",
    "y_pred = km.predict(X)\n",
    "\n",
    "# Tabulate the document and predicted cluster\n",
    "table_data = [[\"Document\", \"Predicted Cluster\"]]\n",
    "table_data.extend([[doc, cluster] for doc, cluster in zip(preprocessed_dataset, y_pred)])\n",
    "print(tabulate(table_data, headers=\"firstrow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c80fb149-054e-4917-a409-7e9ab00733c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purity: 0.6\n"
     ]
    }
   ],
   "source": [
    "# Calculate purity\n",
    "total_samples = len(y_pred)\n",
    "cluster_label_counts = [Counter(y_pred)]\n",
    "purity = sum(max(cluster.values()) for cluster in cluster_label_counts) / total_samples\n",
    "print(\"Purity:\", purity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
